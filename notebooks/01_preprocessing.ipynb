{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Characteristics & Challenge\n",
        "**Type:** The dataset is **Tabular Data** representing clinical records.\n",
        "**Challenge:** High missingness (~90% in some features).\n",
        "**Solution:** We filtered columns with >90% missing values and rows with no data. For remaining missing values, we used **Median Imputation** because clinical variables (like CRP) often have skewed distributions (outliers), making the mean unreliable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Scaling (Z-Score)\n",
        "**Goal:** Prepare data for Clustering.\n",
        "**Reasoning:** K-Means calculates **Euclidean Distances**. Without scaling, features with large values (e.g., Platelets ~200,000) would dominate features with small values (e.g., Creatinine ~1.0).\n",
        "**Action:** Applied `StandardScaler` to transform all features to mean=0 and std=1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patients retained: 603\n",
            "Features retained: 40\n",
            "Data were standardized (z-score) prior to saving.\n",
            "Saved cleaned data to ..\\data\\processed\\01_cleaned_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "RAW_PATH = Path(\"../data/raw/dataset.xlsx\")\n",
        "OUT_PATH = Path(\"../data/processed/01_cleaned_data.csv\")\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load\n",
        "raw = pd.read_excel(RAW_PATH)\n",
        "# Normalize column names\n",
        "raw.columns = (raw.columns\n",
        "               .str.replace(u\"\\xa0\", \" \", regex=False)\n",
        "               .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "               .str.strip())\n",
        "\n",
        "if \"Patient ID\" not in raw.columns:\n",
        "    raise KeyError(\"'Patient ID' column is required.\")\n",
        "\n",
        "# Convert non-ID to numeric\n",
        "non_id_cols = [c for c in raw.columns if c != \"Patient ID\"]\n",
        "raw[non_id_cols] = raw[non_id_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Deduplicate by patient ID\n",
        "df = raw.drop_duplicates(subset=[\"Patient ID\"]).copy()\n",
        "feature_cols = [c for c in df.columns if c != \"Patient ID\"]\n",
        "\n",
        "# Patient filter: >=10 non-missing labs\n",
        "patient_counts = df[feature_cols].notna().sum(axis=1)\n",
        "df = df.loc[patient_counts >= 10].copy()\n",
        "\n",
        "# Feature filter: <80% missing\n",
        "missing_frac = df[feature_cols].isna().mean()\n",
        "kept_features = missing_frac[missing_frac < 0.80].index.tolist()\n",
        "final_cols = [\"Patient ID\"] + kept_features\n",
        "\n",
        "df = df[final_cols].copy()\n",
        "\n",
        "# Impute + scale numeric features\n",
        "numeric_cols = [c for c in df.columns if c != \"Patient ID\"]\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "scaler = StandardScaler()\n",
        "\n",
        "imputed = imputer.fit_transform(df[numeric_cols])\n",
        "scaled = scaler.fit_transform(imputed)\n",
        "\n",
        "processed = df.copy()\n",
        "processed[numeric_cols] = scaled\n",
        "\n",
        "print(f\"Patients retained: {processed.shape[0]}\")\n",
        "print(f\"Features retained: {len(numeric_cols)}\")\n",
        "print(\"Data were standardized (z-score) prior to saving.\")\n",
        "\n",
        "processed.to_csv(OUT_PATH, index=False)\n",
        "print(f\"Saved cleaned data to {OUT_PATH}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data funnel and rationale (Quality over Quantity)\n",
        "- **Funnel:** Initial patients → deduplicated → retained with ≤90% missing → final saved. See printed counts above for the exact numbers in this run.\n",
        "- **Why discard ~90% missing profiles?** Patients with extremely sparse labs lack clinical signal (risk of garbage-in/garbage-out). We keep only those with a rich clinical profile (blood counts + metabolites) to ensure valid downstream inference.\n",
        "- **Imputation is minimal:** Median imputation applied only after filtering to the retained patients/features; no broad filling of empty records.\n",
        "- **Standardization:** Features are z-score scaled (StandardScaler) before export to align ranges for PCA/KMeans.\n",
        "- **Handoff:** Cleaned, scaled dataframe is saved to `../data/processed/01_cleaned_data.csv` for the next notebook.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
